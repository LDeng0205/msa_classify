{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from Bio import SeqIO\n",
    "from model import CouplingsModel\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. sample from MCMC with a markov process\n",
    "###     Underlying distribution: AA only depends on previous position. Probability bigram matrix of alphabets based on war_and_peace.txt\n",
    "### 2. Create a2m files from the samples\n",
    "### 3. Train a potts model with plmc\n",
    "### 4. Observe dependency matrices\n",
    "np.random.seed(0)\n",
    "ALPHABET_PROTEIN = 'ACDEFGHIKLMNPQRSTVWY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "def encode(seqs, alphabet=ALPHABET_PROTEIN):\n",
    "    '''\n",
    "    Go from letters to numbers\n",
    "    '''\n",
    "    aa_to_i = OrderedDict((aa, i) for i, aa in enumerate( alphabet ))\n",
    "    X = np.asarray([[aa_to_i[x] for x in seq] \n",
    "                    for seq in seqs])\n",
    "    return X, aa_to_i\n",
    "def one_hot_encode(s):\n",
    "    return np.eye(21)[s].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### first learn the conditional probabilities\n",
    "input_file = \"war_and_peace.txt\"\n",
    "\n",
    "def build_bigram_freq_matrix(input_file):\n",
    "    \"\"\"\n",
    "    Builds a matrix that represents the transitional\n",
    "    probabilities between letters in input_file.\n",
    "    \n",
    "    bigram_freq_matrix[0][1] is the probability of\n",
    "    transitioning from the 0th letter of the alphabet\n",
    "    to the 1st letter of the alphabet, where letters\n",
    "    are zero-indexed. ' ' (space) is denoted as the\n",
    "    26th letter of the alphabet.\n",
    "    \"\"\"\n",
    "    counts = np.ones([27, 27])\n",
    "    with open(input_file, 'r', encoding='utf8') as f:\n",
    "        for _ in range(100000):\n",
    "            line = f.readline()\n",
    "            if len(line) > 2:\n",
    "                for i in range(len(line) - 2):\n",
    "                    first_char = ord(line[i].upper()) - 65 if line[i].isalpha() else 26\n",
    "                    second_char = ord(line[i+1].upper()) - 65 if line[i+1].isalpha() else 26\n",
    "                    if not (first_char == 26 and second_char == 26) and first_char <= 26 and second_char <= 26:\n",
    "                        counts[first_char][second_char] += 1\n",
    "        bigram_freq_matrix = (counts.T / np.sum(counts, axis=1)).T\n",
    "    return bigram_freq_matrix\n",
    "\n",
    "freq_matrix = build_bigram_freq_matrix(input_file)\n",
    "\n",
    "### freq_matrix is based on all 26 letters of the english alphabet + space\n",
    "### map the matrix to protein alphabets\n",
    "indices = []\n",
    "for aa in list(ALPHABET_PROTEIN):\n",
    "    if aa == '-':\n",
    "        indices.append(26)\n",
    "    else:\n",
    "        indices.append(ord(aa)-ord('A'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f57fc5045b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### only take letters in the protein alphabet\n",
    "\n",
    "protein_freq_matrix = freq_matrix[indices]\n",
    "protein_freq_matrix = protein_freq_matrix[:, indices]\n",
    "\n",
    "### normalize it so probabilities sum to 1\n",
    "for x in protein_freq_matrix:\n",
    "    x /= np.sum(x)\n",
    "\n",
    "### check for valid probabilities\n",
    "\n",
    "assert np.all(protein_freq_matrix >= 0) and np.all(protein_freq_matrix <= 1)\n",
    "plt.imshow(protein_freq_matrix, cmap='binary', interpolation='none')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(bigram, length, first_letter):\n",
    "    seq = first_letter\n",
    "    cur_letter = first_letter\n",
    "    while len(seq) < length:\n",
    "        # iteratively append to the sequence according to bigram frequency\n",
    "        bigram_freq = bigram[ALPHABET_PROTEIN.index(cur_letter)]\n",
    "        \n",
    "        # draw next letter from the given bigram distribution\n",
    "        next_letter = np.random.choice(list(ALPHABET_PROTEIN), p=bigram_freq)\n",
    "        seq += next_letter\n",
    "        cur_letter = next_letter\n",
    "    return seq\n",
    "\n",
    "def draw_samples_MP(bigram, length, n):\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        random_first_letter = np.random.choice(list(ALPHABET_PROTEIN))\n",
    "        samples.append(sample(bigram, length, random_first_letter))\n",
    "    return samples\n",
    "\n",
    "def sample_two_letters(bigram, length, first_letter):\n",
    "    seq = first_letter\n",
    "    cur_letter = first_letter\n",
    "    while len(seq) < length:\n",
    "        # iteratively append to the sequence according to bigram frequency\n",
    "        bigram_freq = bigram[(\"AB\").index(cur_letter)]\n",
    "        \n",
    "        # draw next letter from the given bigram distribution\n",
    "        next_letter = np.random.choice(list(\"AB\"), p=bigram_freq)\n",
    "        seq += next_letter\n",
    "        cur_letter = next_letter\n",
    "    return seq\n",
    "\n",
    "def draw_samples_MP_two_letters(bigram, length, n):\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        random_first_letter = np.random.choice(list(\"AB\"))\n",
    "        samples.append(sample_two_letters(bigram, length, random_first_letter))\n",
    "    return samples\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the potts model on generated samples. This is to confirm the potts model is learning pairwise weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio.Seq import Seq\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "\n",
    "### generate data\n",
    "samples = draw_samples_MP(protein_freq_matrix, 20, 100000)\n",
    "\n",
    "### convert the generated samples to fasta format\n",
    "records = (SeqRecord(Seq(''.join(seq)), str(index)) for index,seq in enumerate(samples) )\n",
    "with open(\"simulation/markov.a2m\", \"w\") as output_handle:\n",
    "    SeqIO.write(records, output_handle, \"fasta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### count empiracally the bigram frequencies; sanity check\n",
    "\n",
    "empirical_bigram = np.zeros((20, 20))\n",
    "print(empirical_bigram.shape)\n",
    "for s in samples:\n",
    "    for i in range(len(s) - 1):\n",
    "        empirical_bigram[ALPHABET_PROTEIN.index(s[i])][ALPHABET_PROTEIN.index(s[i+1])] += 1\n",
    "plt.imshow(empirical_bigram, cmap='binary', interpolation='none')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found focus 0 as sequence 1\n",
      "100000 valid sequences out of 100000 \n",
      "20 sites out of 20\n",
      "Effective number of samples (to 1 decimal place): 99998.0\t(80% identical neighborhood = 1.000 samples)\n",
      "iter\ttime\tcond\tfx\t-loglk\t||h||\t||e||\n",
      "1\t0.2\t2693.13\t4892182.2\t4892081.3\t24.3\t5.0\n",
      "2\t0.2\t3700.57\t4338860.9\t4334157.5\t25.1\t35.2\n",
      "3\t0.3\t2134.96\t4310221.5\t4301032.1\t24.0\t49.2\n",
      "4\t0.3\t1189.37\t3980571.8\t3973994.8\t23.9\t41.6\n",
      "5\t0.4\t1144.06\t3906889.4\t3902385.6\t23.9\t34.4\n",
      "6\t0.5\t483.32\t3852524.3\t3847025.7\t23.7\t38.0\n",
      "7\t0.5\t478.65\t3812182.6\t3805875.1\t23.6\t40.7\n",
      "8\t0.6\t568.55\t3697179.1\t3687016.7\t23.2\t51.7\n",
      "9\t0.7\t601.62\t3668911.4\t3657184.7\t23.2\t55.5\n",
      "10\t0.7\t360.55\t3633448.7\t3620200.6\t23.3\t59.0\n",
      "11\t0.8\t349.47\t3605054.5\t3589997.9\t23.6\t62.9\n",
      "12\t0.8\t318.45\t3586544.4\t3570902.9\t23.8\t64.1\n",
      "13\t0.9\t273.64\t3565954.4\t3549628.9\t24.0\t65.5\n",
      "14\t0.9\t290.10\t3536536.7\t3518714.9\t24.1\t68.5\n",
      "15\t1.1\t220.60\t3523873.8\t3504610.0\t24.1\t71.2\n",
      "16\t1.1\t168.42\t3514010.1\t3494505.4\t24.0\t71.6\n",
      "17\t1.2\t168.79\t3498615.7\t3478236.6\t23.8\t73.2\n",
      "18\t1.2\t244.39\t3487975.4\t3466117.5\t23.9\t75.8\n",
      "19\t1.3\t141.96\t3476328.2\t3453126.0\t24.0\t78.1\n",
      "20\t1.3\t117.36\t3466252.5\t3441216.7\t24.2\t81.2\n",
      "21\t1.4\t138.63\t3460088.8\t3433574.2\t24.3\t83.5\n",
      "22\t1.4\t101.24\t3453747.0\t3425973.1\t24.4\t85.5\n",
      "23\t1.5\t151.46\t3447239.6\t3417641.9\t24.5\t88.2\n",
      "24\t1.5\t93.03\t3442617.2\t3412029.3\t24.5\t89.7\n",
      "25\t1.6\t72.69\t3440424.2\t3410000.7\t24.4\t89.5\n",
      "26\t1.6\t68.04\t3435508.0\t3404650.2\t24.4\t90.1\n",
      "27\t1.7\t133.41\t3432655.8\t3400210.5\t24.4\t92.4\n",
      "28\t1.7\t70.46\t3429190.1\t3396031.3\t24.5\t93.4\n",
      "29\t1.8\t50.28\t3426488.2\t3392140.1\t24.6\t95.1\n",
      "30\t1.8\t49.74\t3424596.0\t3389422.5\t24.6\t96.2\n",
      "31\t1.9\t97.24\t3422724.0\t3386677.6\t24.7\t97.4\n",
      "32\t2.0\t46.98\t3420084.9\t3383151.8\t24.7\t98.6\n",
      "33\t2.0\t40.91\t3418574.9\t3381520.0\t24.8\t98.7\n",
      "34\t2.1\t70.80\t3416764.4\t3379487.5\t24.8\t99.0\n",
      "35\t2.2\t45.18\t3415349.2\t3378101.1\t24.9\t99.0\n",
      "36\t2.2\t37.11\t3414299.8\t3376995.5\t24.9\t99.1\n",
      "37\t2.3\t42.66\t3412662.2\t3375036.4\t24.9\t99.5\n",
      "38\t2.3\t81.68\t3412158.7\t3373989.9\t25.0\t100.2\n",
      "39\t2.4\t34.60\t3411008.5\t3372812.8\t25.0\t100.2\n",
      "40\t2.4\t26.10\t3410323.7\t3371962.9\t25.0\t100.5\n",
      "41\t2.5\t30.52\t3409710.1\t3371084.6\t25.0\t100.8\n",
      "42\t2.5\t41.18\t3408596.1\t3369446.7\t25.1\t101.5\n",
      "43\t2.6\t76.94\t3408461.3\t3368537.8\t25.2\t102.5\n",
      "44\t2.6\t24.17\t3407248.5\t3367434.5\t25.2\t102.4\n",
      "45\t2.7\t20.13\t3406930.2\t3367151.3\t25.2\t102.3\n",
      "46\t2.7\t31.65\t3406302.7\t3366377.1\t25.2\t102.5\n",
      "47\t2.8\t46.89\t3405864.5\t3365600.2\t25.3\t102.9\n",
      "48\t2.9\t23.06\t3405388.8\t3364986.9\t25.3\t103.1\n",
      "49\t2.9\t20.56\t3404976.8\t3364329.8\t25.4\t103.4\n",
      "50\t3.0\t21.43\t3404645.6\t3363779.1\t25.4\t103.7\n",
      "51\t3.0\t66.33\t3404411.1\t3363075.9\t25.6\t104.3\n",
      "52\t3.1\t19.91\t3403732.5\t3362280.0\t25.6\t104.4\n",
      "53\t3.1\t14.80\t3403520.6\t3362098.8\t25.6\t104.4\n",
      "54\t3.2\t18.54\t3403240.2\t3361769.6\t25.6\t104.5\n",
      "55\t3.2\t29.14\t3402902.5\t3361259.6\t25.7\t104.7\n",
      "56\t3.3\t20.22\t3402562.0\t3360654.5\t25.8\t105.0\n",
      "57\t3.4\t14.44\t3402347.4\t3360310.5\t25.8\t105.2\n",
      "58\t3.4\t17.86\t3402108.6\t3359856.7\t25.9\t105.4\n",
      "59\t3.5\t21.45\t3401909.6\t3359444.2\t26.0\t105.7\n",
      "60\t3.5\t16.16\t3401649.8\t3358948.6\t26.0\t106.0\n",
      "61\t3.6\t21.71\t3401369.5\t3358329.0\t26.1\t106.4\n",
      "62\t3.6\t18.36\t3401171.7\t3357852.1\t26.2\t106.8\n",
      "63\t3.7\t11.89\t3401036.1\t3357721.3\t26.2\t106.8\n",
      "64\t3.7\t11.57\t3400835.7\t3357434.9\t26.2\t106.9\n",
      "65\t3.8\t17.87\t3400676.4\t3357071.6\t26.3\t107.1\n",
      "66\t3.8\t13.17\t3400467.8\t3356585.9\t26.4\t107.5\n",
      "67\t3.9\t16.74\t3400324.8\t3356195.5\t26.5\t107.8\n",
      "68\t3.9\t11.06\t3400185.3\t3355888.6\t26.6\t108.0\n",
      "69\t4.0\t11.07\t3400083.1\t3355717.3\t26.6\t108.0\n",
      "70\t4.0\t14.96\t3399960.8\t3355461.2\t26.6\t108.2\n",
      "71\t4.1\t11.70\t3399861.1\t3355248.4\t26.7\t108.3\n",
      "72\t4.1\t9.76\t3399783.5\t3355094.0\t26.7\t108.4\n",
      "73\t4.2\t10.14\t3399689.7\t3354863.3\t26.7\t108.6\n",
      "74\t4.2\t18.96\t3399631.0\t3354595.5\t26.8\t108.9\n",
      "75\t4.2\t8.66\t3399552.2\t3354493.9\t26.8\t108.9\n",
      "76\t4.3\t7.54\t3399500.1\t3354383.9\t26.9\t109.0\n",
      "77\t4.4\t8.69\t3399449.4\t3354256.5\t26.9\t109.0\n",
      "78\t4.4\t15.29\t3399351.4\t3353931.6\t27.0\t109.3\n",
      "79\t4.4\t11.80\t3399286.8\t3353691.3\t27.1\t109.5\n",
      "80\t4.5\t6.55\t3399238.6\t3353664.3\t27.0\t109.5\n",
      "81\t4.5\t6.75\t3399186.6\t3353601.8\t27.1\t109.5\n",
      "82\t4.6\t7.71\t3399139.4\t3353499.3\t27.1\t109.6\n",
      "83\t4.7\t13.01\t3399100.5\t3353371.5\t27.1\t109.7\n",
      "84\t4.7\t7.02\t3399045.9\t3353227.1\t27.2\t109.8\n",
      "85\t4.8\t5.81\t3399004.0\t3353104.1\t27.2\t109.9\n",
      "86\t4.8\t7.10\t3398956.8\t3352964.2\t27.3\t110.0\n",
      "87\t4.9\t14.30\t3398930.9\t3352818.8\t27.3\t110.1\n",
      "88\t4.9\t5.73\t3398883.7\t3352778.5\t27.3\t110.1\n",
      "89\t5.0\t5.48\t3398854.5\t3352762.4\t27.3\t110.1\n",
      "90\t5.0\t7.17\t3398819.6\t3352716.6\t27.4\t110.1\n",
      "91\t5.1\t7.41\t3398762.2\t3352620.2\t27.4\t110.2\n",
      "92\t5.2\t9.39\t3398736.3\t3352543.6\t27.5\t110.2\n",
      "93\t5.2\t4.62\t3398707.0\t3352483.2\t27.5\t110.3\n",
      "94\t5.3\t4.39\t3398687.5\t3352440.7\t27.5\t110.3\n",
      "95\t5.3\t6.28\t3398663.6\t3352388.3\t27.5\t110.3\n",
      "96\t5.4\t9.82\t3398635.3\t3352309.9\t27.6\t110.4\n",
      "97\t5.4\t5.78\t3398602.5\t3352255.4\t27.6\t110.4\n",
      "98\t5.5\t4.53\t3398575.6\t3352220.5\t27.6\t110.4\n",
      "99\t5.5\t5.11\t3398555.1\t3352187.3\t27.7\t110.5\n",
      "100\t5.6\t6.79\t3398537.5\t3352149.7\t27.7\t110.5\n",
      "101\t5.7\t4.03\t3398519.3\t3352124.5\t27.7\t110.5\n",
      "102\t5.7\t4.52\t3398498.9\t3352095.6\t27.7\t110.5\n",
      "103\t5.7\t5.13\t3398475.4\t3352059.6\t27.8\t110.5\n",
      "104\t5.8\t13.48\t3398472.0\t3352035.7\t27.9\t110.5\n",
      "105\t5.8\t3.86\t3398437.9\t3351999.1\t27.9\t110.5\n",
      "106\t5.9\t3.25\t3398428.9\t3351992.0\t27.9\t110.5\n",
      "107\t5.9\t4.16\t3398411.3\t3351975.4\t27.9\t110.5\n",
      "108\t6.0\t10.74\t3398403.4\t3351964.6\t27.9\t110.5\n",
      "109\t6.0\t4.04\t3398382.4\t3351944.4\t28.0\t110.5\n",
      "110\t6.1\t2.89\t3398372.1\t3351933.7\t28.0\t110.5\n",
      "111\t6.1\t3.59\t3398362.0\t3351922.9\t28.0\t110.5\n",
      "112\t6.2\t4.16\t3398345.2\t3351905.7\t28.0\t110.5\n",
      "113\t6.3\t6.17\t3398333.7\t3351894.6\t28.1\t110.5\n",
      "114\t6.3\t3.10\t3398317.6\t3351879.1\t28.1\t110.5\n",
      "115\t6.4\t2.96\t3398307.3\t3351870.7\t28.1\t110.5\n",
      "116\t6.4\t5.12\t3398295.5\t3351863.3\t28.2\t110.5\n",
      "117\t6.5\t4.12\t3398285.1\t3351857.0\t28.2\t110.5\n",
      "118\t6.5\t3.07\t3398277.4\t3351851.0\t28.2\t110.5\n",
      "119\t6.6\t3.03\t3398261.4\t3351841.1\t28.2\t110.5\n",
      "120\t6.6\t5.62\t3398253.4\t3351838.7\t28.2\t110.5\n",
      "121\t6.7\t3.24\t3398243.2\t3351832.7\t28.3\t110.5\n",
      "122\t6.7\t2.56\t3398234.1\t3351827.8\t28.3\t110.5\n",
      "123\t6.8\t3.88\t3398227.4\t3351825.8\t28.3\t110.5\n",
      "124\t6.8\t3.06\t3398219.4\t3351823.4\t28.3\t110.5\n",
      "125\t6.9\t2.62\t3398211.3\t3351821.3\t28.4\t110.5\n",
      "126\t6.9\t4.72\t3398205.5\t3351823.9\t28.4\t110.5\n",
      "127\t7.0\t2.63\t3398199.0\t3351819.7\t28.4\t110.5\n",
      "128\t7.0\t2.65\t3398192.9\t3351817.3\t28.4\t110.5\n",
      "129\t7.1\t2.90\t3398186.3\t3351816.5\t28.4\t110.5\n",
      "130\t7.1\t5.94\t3398178.3\t3351821.1\t28.5\t110.4\n",
      "131\t7.2\t2.58\t3398168.7\t3351819.4\t28.5\t110.4\n",
      "132\t7.3\t1.97\t3398163.9\t3351817.2\t28.5\t110.4\n",
      "133\t7.3\t2.93\t3398158.4\t3351817.2\t28.6\t110.4\n",
      "134\t7.4\t3.24\t3398153.9\t3351819.9\t28.6\t110.4\n",
      "135\t7.4\t2.37\t3398148.9\t3351819.3\t28.6\t110.4\n",
      "136\t7.5\t2.21\t3398141.0\t3351820.7\t28.6\t110.4\n",
      "137\t7.5\t3.60\t3398136.6\t3351824.5\t28.7\t110.4\n",
      "138\t7.6\t2.26\t3398131.8\t3351824.5\t28.7\t110.4\n",
      "139\t7.6\t1.91\t3398126.9\t3351826.2\t28.7\t110.4\n",
      "140\t7.7\t2.38\t3398123.4\t3351827.8\t28.7\t110.4\n",
      "141\t7.7\t2.28\t3398117.4\t3351831.7\t28.8\t110.4\n",
      "142\t7.8\t6.21\t3398115.5\t3351845.2\t28.8\t110.3\n",
      "143\t7.8\t2.03\t3398107.8\t3351838.0\t28.8\t110.3\n",
      "144\t7.9\t1.59\t3398105.1\t3351835.9\t28.8\t110.3\n",
      "145\t8.0\t1.90\t3398101.0\t3351836.4\t28.8\t110.3\n",
      "146\t8.0\t4.49\t3398098.1\t3351840.0\t28.9\t110.3\n",
      "147\t8.1\t1.95\t3398093.9\t3351839.5\t28.9\t110.3\n",
      "148\t8.1\t1.51\t3398091.5\t3351839.1\t28.9\t110.3\n",
      "149\t8.2\t1.83\t3398088.7\t3351838.6\t28.9\t110.3\n",
      "150\t8.2\t3.53\t3398084.9\t3351837.5\t29.0\t110.3\n",
      "151\t8.3\t1.90\t3398080.6\t3351834.8\t29.0\t110.3\n",
      "152\t8.3\t1.47\t3398077.6\t3351830.8\t29.0\t110.3\n",
      "153\t8.4\t1.94\t3398074.0\t3351824.3\t29.0\t110.3\n",
      "154\t8.4\t2.85\t3398071.7\t3351817.9\t29.1\t110.3\n",
      "155\t8.5\t1.61\t3398068.8\t3351813.4\t29.1\t110.3\n",
      "156\t8.5\t1.51\t3398065.7\t3351807.5\t29.1\t110.3\n",
      "157\t8.6\t1.64\t3398063.2\t3351801.8\t29.1\t110.3\n",
      "158\t8.6\t3.84\t3398061.2\t3351790.6\t29.2\t110.3\n",
      "159\t8.7\t1.44\t3398057.3\t3351784.3\t29.2\t110.3\n",
      "160\t8.7\t1.28\t3398055.7\t3351781.9\t29.2\t110.3\n",
      "161\t8.8\t1.58\t3398052.8\t3351774.4\t29.2\t110.3\n",
      "162\t8.9\t2.30\t3398051.2\t3351768.1\t29.2\t110.4\n",
      "163\t9.0\t1.52\t3398049.1\t3351761.2\t29.3\t110.4\n",
      "164\t9.0\t1.54\t3398046.7\t3351752.3\t29.3\t110.4\n",
      "165\t9.1\t2.57\t3398045.1\t3351743.5\t29.3\t110.4\n",
      "166\t9.1\t1.43\t3398043.4\t3351740.9\t29.3\t110.4\n",
      "167\t9.2\t1.34\t3398041.3\t3351737.0\t29.3\t110.4\n",
      "168\t9.2\t1.59\t3398039.6\t3351732.4\t29.3\t110.4\n",
      "169\t9.3\t3.72\t3398037.8\t3351720.6\t29.4\t110.4\n",
      "170\t9.3\t1.44\t3398034.8\t3351713.8\t29.4\t110.4\n",
      "171\t9.4\t1.08\t3398033.5\t3351710.2\t29.4\t110.4\n",
      "172\t9.4\t1.23\t3398032.1\t3351704.6\t29.4\t110.4\n",
      "173\t9.5\t1.40\t3398030.0\t3351696.3\t29.5\t110.4\n",
      "174\t9.6\t2.15\t3398028.6\t3351688.7\t29.5\t110.4\n",
      "175\t9.6\t1.16\t3398026.8\t3351682.2\t29.5\t110.4\n",
      "176\t9.7\t1.14\t3398025.3\t3351677.9\t29.6\t110.4\n",
      "177\t9.7\t2.14\t3398023.7\t3351672.1\t29.6\t110.4\n",
      "178\t9.8\t1.40\t3398021.9\t3351667.9\t29.6\t110.4\n",
      "179\t9.9\t1.13\t3398020.3\t3351665.0\t29.6\t110.4\n",
      "180\t9.9\t1.84\t3398018.7\t3351661.6\t29.7\t110.4\n",
      "181\t10.0\t1.45\t3398017.3\t3351658.6\t29.7\t110.4\n",
      "182\t10.0\t1.24\t3398016.0\t3351656.2\t29.7\t110.4\n",
      "183\t10.1\t1.67\t3398013.5\t3351650.6\t29.8\t110.4\n",
      "184\t10.1\t3.02\t3398013.0\t3351646.2\t29.8\t110.5\n",
      "185\t10.2\t1.11\t3398011.1\t3351645.5\t29.8\t110.4\n",
      "186\t10.2\t0.85\t3398010.1\t3351644.6\t29.8\t110.4\n",
      "187\t10.3\t1.14\t3398009.0\t3351642.9\t29.8\t110.5\n",
      "188\t10.3\t1.36\t3398007.4\t3351640.9\t29.9\t110.5\n",
      "189\t10.4\t1.99\t3398006.0\t3351638.9\t29.9\t110.5\n",
      "190\t10.5\t1.05\t3398004.4\t3351637.8\t30.0\t110.5\n",
      "191\t10.5\t1.06\t3398003.1\t3351638.0\t30.0\t110.4\n",
      "192\t10.6\t1.51\t3398001.5\t3351639.1\t30.0\t110.4\n",
      "193\t10.6\t2.30\t3398000.3\t3351642.6\t30.1\t110.4\n",
      "194\t10.7\t1.09\t3397998.8\t3351642.2\t30.1\t110.4\n",
      "195\t10.7\t1.00\t3397997.7\t3351642.8\t30.1\t110.4\n",
      "196\t10.8\t1.15\t3397996.7\t3351644.4\t30.1\t110.4\n",
      "197\t10.8\t2.33\t3397995.6\t3351650.0\t30.2\t110.4\n",
      "198\t10.9\t1.02\t3397994.1\t3351651.9\t30.2\t110.4\n",
      "199\t10.9\t0.91\t3397993.3\t3351652.6\t30.2\t110.4\n",
      "200\t11.0\t1.04\t3397991.9\t3351655.5\t30.2\t110.4\n",
      "Gradient optimization: MAXIMUMITERATION\n"
     ]
    }
   ],
   "source": [
    "### training the potts model\n",
    "!plmc/bin/plmc -o simulation/markov.model_params -c simulation/markov.txt -f 0 -le 3.8 -lh 0.01 -m 200 -t 0.2 -g simulation/markov.a2m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### observe the pairwise weight matrices in the Potts model\n",
    "c = CouplingsModel(f\"simulation/markov.model_params\")\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        plt.subplot(3, 3, 3*i+j+1)\n",
    "        plt.imshow(c.J_ij[i][j], cmap='binary', interpolation='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = samples\n",
    "encoded_data, _ = encode(data)\n",
    "encoded_data = [one_hot_encode(x) for x in encoded_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneClassSVM(degree=2, kernel=&#x27;poly&#x27;, nu=0.3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneClassSVM</label><div class=\"sk-toggleable__content\"><pre>OneClassSVM(degree=2, kernel=&#x27;poly&#x27;, nu=0.3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "OneClassSVM(degree=2, kernel='poly', nu=0.3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "clf = OneClassSVM(kernel='poly', degree=2, nu=0.3)\n",
    "train = encoded_data[:70000]\n",
    "clf.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(negatives, positives):\n",
    "    pred = clf.predict(positives)\n",
    "    \n",
    "    enc_n, _ = encode(negatives)\n",
    "    enc_n = [one_hot_encode(x) for x in enc_n]\n",
    "    pred_n = clf.predict(enc_n)\n",
    "    correct = np.sum(pred[np.where(pred==1)]) - np.sum(pred_n[np.where(pred_n==-1)])\n",
    "    print(np.sum(pred_n[np.where(pred_n==-1)]))\n",
    "    print(np.sum(pred[np.where(pred==1)]))\n",
    "    return correct/(len(negatives) + len(positives))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-8840\n",
      "19662\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.71255"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negatives = np.random.choice(list(ALPHABET_PROTEIN), size=(10000, 3))\n",
    "eval(negatives, encoded_data[70000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['N', 'E', 'R'], dtype='<U1')"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# modify energy computation to enable computing sequences with gap characters\n",
    "def mod_convert_sequences(sequences, c):\n",
    "    \"\"\"\n",
    "    Converts sequences in string format into internal symbol representation\n",
    "    according to alphabet of model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequences : list of str\n",
    "        List of sequences (must have same length and correspond to\n",
    "        model states)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        Matrix of size len(sequences) x L of sequences converted to\n",
    "        integer symbols\n",
    "    \"\"\"\n",
    "    seq_lens = list(set(map(len, sequences)))\n",
    "    if len(seq_lens) != 1:\n",
    "        raise ValueError(\"Input sequences have different lengths: \" + str(seq_lens))\n",
    "\n",
    "    L_seq = seq_lens[0]\n",
    "\n",
    "    S = np.empty((len(sequences), L_seq), dtype=int)\n",
    "\n",
    "    try:\n",
    "        for i, s in enumerate(sequences):\n",
    "            conv = []\n",
    "            for x in s:\n",
    "                if x == '-':\n",
    "                    conv.append(-1)\n",
    "                else:\n",
    "                    conv.append(c.alphabet_map[x])\n",
    "            S[i] = conv\n",
    "    except KeyError:\n",
    "        raise ValueError(\"Invalid symbol in sequence {}: {}\".format(i, x))\n",
    "    return S\n",
    "    \n",
    "def mod_hamiltonians(sequences, J_ij, h_i):\n",
    "    \"\"\"\n",
    "    Calculates the Hamiltonian of the global probability distribution P(A_1, ..., A_L)\n",
    "    for a given sequence A_1,...,A_L from J_ij and h_i parameters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    sequences : np.array\n",
    "        Sequence matrix for which Hamiltonians will be computed\n",
    "    J_ij: np.array\n",
    "        L x L x num_symbols x num_symbols J_ij pair coupling parameter matrix\n",
    "    h_i: np.array\n",
    "        L x num_symbols h_i fields parameter matrix\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.array\n",
    "        Float matrix of size len(sequences) x 3, where each row corresponds to the\n",
    "        1) total Hamiltonian of sequence and the 2) J_ij and 3) h_i sub-sums\n",
    "    \"\"\"\n",
    "    # iterate over sequences\n",
    "    N, L = sequences.shape\n",
    "    H = np.zeros((N, 3))\n",
    "    for s in range(N):\n",
    "        A = sequences[s]\n",
    "        hi_sum = 0.0\n",
    "        Jij_sum = 0.0\n",
    "        for i in range(L):\n",
    "            if A[i] != -1:\n",
    "                hi_sum += h_i[i, A[i]]\n",
    "                for j in range(i + 1, L):\n",
    "                    if A[j] != -1:\n",
    "                        Jij_sum += J_ij[i, j, A[i], A[j]]\n",
    "\n",
    "        H[s] = [Jij_sum + hi_sum, Jij_sum, hi_sum]\n",
    "\n",
    "    return H\n",
    "\n",
    "def energies_from_model(c, sequences):\n",
    "    S = mod_convert_sequences(sequences.tolist(), c)\n",
    "    msa_energies = mod_hamiltonians(S, c.J_ij, c.h_i)\n",
    "    return msa_energies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = energies_from_model(c, np.asarray(data[70000:]))[:, 0]\n",
    "neg = energies_from_model(c, np.asarray(negatives))[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2978668361902237"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=c.hamiltonians([c.seq()])[0][0]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13298,)\n",
      "(2127,)\n"
     ]
    }
   ],
   "source": [
    "print(pos[np.where(pos >= x)].shape)\n",
    "print(neg[np.where(neg >= x)].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein_freq_matrix.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 5.1: Ensure that SVM works on a somewhat similar problem. The main idea is having two different methods of generating sequences with pre-encoded pairwise frequences, and train a one class SVM on one set of the sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "np.random.seed(0)\n",
    "training_seq = draw_samples_MP(protein_freq_matrix, seq_len, 10000)\n",
    "\n",
    "negative_matrix = protein_freq_matrix[np.random.permutation(protein_freq_matrix.shape[0])]\n",
    "\n",
    "negatives = draw_samples_MP(negative_matrix, seq_len, 1000)\n",
    "positives = draw_samples_MP(protein_freq_matrix, seq_len, 1000)\n",
    "\n",
    "pos_perm = np.random.permutation(seq_len)\n",
    "neg_perm = np.random.permutation(seq_len)\n",
    "\n",
    "\n",
    "\n",
    "assert not np.allclose(pos_perm, neg_perm)\n",
    "assert not np.allclose(negative_matrix, protein_freq_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_seq, _ = encode(training_seq)\n",
    "training_seq = np.asarray([x[pos_perm] for x in training_seq])\n",
    "mutation = [np.copy(training_seq[0]) for _ in range(1000)]\n",
    "for i in range(len(mutation)):\n",
    "    mutation[i][i%training_seq.shape[1]] = training_seq[i][i%training_seq.shape[1]]\n",
    "training_seq = np.asarray([one_hot_encode(x) for x in training_seq])\n",
    "mutation = np.asarray([one_hot_encode(x) for x in mutation])\n",
    "\n",
    "positives, _ = encode(positives)\n",
    "positives = np.asarray([x[pos_perm] for x in positives])\n",
    "positives = np.asarray([one_hot_encode(x) for x in positives])\n",
    "\n",
    "\n",
    "negatives, _ = encode(negatives)\n",
    "negatives = np.asarray([x[neg_perm] for x in negatives])\n",
    "negatives = np.asarray([one_hot_encode(x) for x in negatives])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_rich(arr):\n",
    "    d = arr.shape[1]\n",
    "    rich = np.zeros((arr.shape[0], d**2))\n",
    "    for seq in range(arr.shape[0]):\n",
    "        for i in range(d):\n",
    "            for j in range(i, d):\n",
    "                if arr[seq][i] == 1 and arr[seq][j] == 1 or i == j:\n",
    "                    rich[seq][i*d + j] = 1\n",
    "    return rich\n",
    "\n",
    "def expand_rich_no_linear(arr):\n",
    "    d = arr.shape[1]\n",
    "    rich = np.zeros((arr.shape[0], d**2))\n",
    "    for seq in range(arr.shape[0]):\n",
    "        for i in range(d):\n",
    "            for j in range(i + 1, d):\n",
    "                if arr[seq][i] == 1 and arr[seq][j] == 1:\n",
    "                    rich[seq][i*d + j] = 1\n",
    "    return rich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_training = expand_rich_no_linear(training_seq)\n",
    "rich_pos = expand_rich_no_linear(positives)\n",
    "rich_neg = expand_rich_no_linear(negatives)\n",
    "rich_mutation = expand_rich_no_linear(mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 176400)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rich_training.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel: linear, nu: 2e-05, result:  0.4645\n",
      "true pos:  792\n",
      "true neg:  137\n",
      "kernel: linear, nu: 0.0001, result:  0.465\n",
      "true pos:  792\n",
      "true neg:  138\n",
      "kernel: linear, nu: 0.0002, result:  0.465\n",
      "true pos:  792\n",
      "true neg:  138\n",
      "kernel: linear, nu: 0.001, result:  0.465\n",
      "true pos:  792\n",
      "true neg:  138\n",
      "kernel: linear, nu: 0.002, result:  0.465\n",
      "true pos:  792\n",
      "true neg:  138\n",
      "kernel: linear, nu: 0.01, result:  0.465\n",
      "true pos:  792\n",
      "true neg:  138\n",
      "kernel: linear, nu: 0.01, result:  0.465\n",
      "true pos:  792\n",
      "true neg:  138\n",
      "kernel: linear, nu: 0.02, result:  0.465\n",
      "true pos:  792\n",
      "true neg:  138\n",
      "kernel: linear, nu: 0.1, result:  0.461\n",
      "true pos:  784\n",
      "true neg:  138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "for n in [0.00002, 0.0001, 0.0002, 0.001, 0.002, 0.01, 0.01, 0.02, 0.1, 0.2, 0.999]:\n",
    "    clf = OneClassSVM(kernel='linear', nu=n)\n",
    "    clf.fit(rich_training)\n",
    "    pred_pos = clf.predict(rich_pos)\n",
    "    pred_neg = clf.predict(rich_neg)\n",
    "\n",
    "    true_pos = np.count_nonzero(pred_pos == 1)\n",
    "    true_neg = np.count_nonzero(pred_neg != 1)\n",
    "    result = (true_neg + true_pos)/(pred_neg.shape[0] + pred_pos.shape[0])\n",
    "    print(f'kernel: linear, nu: {n}, result: ', result)\n",
    "    print(\"true pos: \", true_pos)\n",
    "    print(\"true neg: \", true_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not np.allclose(rich_mutation[0], rich_mutation[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_t = clf.predict(rich_mutation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "n = 0.15\n",
    "clf2 = OneClassSVM(kernel='rbf', nu=n)\n",
    "clf2.fit(training_seq)\n",
    "pred_pos = clf2.predict(positives)\n",
    "pred_neg = clf2.predict(negatives)\n",
    "\n",
    "true_pos = np.count_nonzero(pred_pos == 1)\n",
    "true_neg = np.count_nonzero(pred_neg != 1)\n",
    "result = (true_neg + true_pos)/(pred_neg.shape[0] + pred_pos.shape[0])\n",
    "print(f'kernel: linear, nu: {n}, result: ', result)\n",
    "print(\"true pos: \", true_pos)\n",
    "print(\"true neg: \", true_neg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "with open('result_simulation.txt', 'a') as file:\n",
    "    date_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    file.write(date_time)\n",
    "    file.write('\\n\\n')\n",
    "    # train a linear model, which should theoretically be similar to the potts model.\n",
    "    for n in [0.001, 0.002, 0.01, 0.02, 0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9]:\n",
    "        clf = OneClassSVM(kernel='linear', nu=n)\n",
    "        clf.fit(training_seq)\n",
    "        pred_pos = clf.predict(positives)\n",
    "        pred_neg = clf.predict(negatives)\n",
    "        \n",
    "        true_pos = np.count_nonzero(pred_pos == 1)\n",
    "        true_neg = np.count_nonzero(pred_neg != 1)\n",
    "        result = (true_neg + true_pos)/(pred_neg.shape[0] + pred_pos.shape[0])\n",
    "        print(f'kernel: linear, nu: {n}, result: ', result)\n",
    "        print(\"true pos: \", true_pos)\n",
    "        print(\"true neg: \", true_neg)\n",
    "        file.write(f'kernel: linear, nu: {n}, result: {result}\\n')\n",
    "        file.write(f'True Positives: {true_pos}, True Negatives: {true_neg}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel: rbf, nu: 0.001, result:  0.545\n",
      "true pos:  976\n",
      "true neg:  114\n",
      "kernel: rbf, nu: 0.002, result:  0.545\n",
      "true pos:  976\n",
      "true neg:  114\n",
      "kernel: rbf, nu: 0.01, result:  0.547\n",
      "true pos:  977\n",
      "true neg:  117\n",
      "kernel: rbf, nu: 0.02, result:  0.548\n",
      "true pos:  968\n",
      "true neg:  128\n",
      "kernel: rbf, nu: 0.1, result:  0.56\n",
      "true pos:  898\n",
      "true neg:  222\n",
      "kernel: rbf, nu: 0.2, result:  0.567\n",
      "true pos:  810\n",
      "true neg:  324\n",
      "kernel: rbf, nu: 0.25, result:  0.556\n",
      "true pos:  740\n",
      "true neg:  372\n",
      "kernel: rbf, nu: 0.3, result:  0.5565\n",
      "true pos:  701\n",
      "true neg:  412\n",
      "kernel: rbf, nu: 0.4, result:  0.5315\n",
      "true pos:  586\n",
      "true neg:  477\n",
      "kernel: rbf, nu: 0.5, result:  0.525\n",
      "true pos:  500\n",
      "true neg:  550\n",
      "kernel: rbf, nu: 0.7, result:  0.4945\n",
      "true pos:  303\n",
      "true neg:  686\n",
      "kernel: rbf, nu: 0.9, result:  0.486\n",
      "true pos:  90\n",
      "true neg:  882\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "with open('result_simulation.txt', 'a') as file:\n",
    "    date_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    file.write(date_time)\n",
    "    file.write('\\n\\n')\n",
    "    # train a linear model, which should theoretically be similar to the potts model.\n",
    "    for n in [0.001, 0.002, 0.01, 0.02, 0.1, 0.2, 0.25, 0.3, 0.4, 0.5, 0.7, 0.9]:\n",
    "        clf = OneClassSVM(kernel='rbf', nu=n)\n",
    "        clf.fit(training_seq)\n",
    "        pred_pos = clf.predict(positives)\n",
    "        pred_neg = clf.predict(negatives)\n",
    "        \n",
    "        true_pos = np.count_nonzero(pred_pos == 1)\n",
    "        true_neg = np.count_nonzero(pred_neg != 1)\n",
    "        result = (true_neg + true_pos)/(pred_neg.shape[0] + pred_pos.shape[0])\n",
    "        print(f'kernel: rbf, nu: {n}, result: ', result)\n",
    "        print(\"true pos: \", true_pos)\n",
    "        print(\"true neg: \", true_neg)\n",
    "        file.write(f'kernel: rbf, nu: {n}, result: {result}\\n')\n",
    "        file.write(f'True Positives: {true_pos}, True Negatives: {true_neg}\\n\\n\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment 5.2: Simplified version of 5.2 with only two letters in the protein alphabet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_samples_MP_three_letters(bigram, length, n):\n",
    "    samples = []\n",
    "    for _ in range(n):\n",
    "        random_first_letter = np.random.choice(list(\"ABC\"))\n",
    "        samples.append(sample_three_letters(bigram, length, random_first_letter))\n",
    "    return samples\n",
    "\n",
    "def sample_three_letters(bigram, length, first_letter):\n",
    "    seq = first_letter\n",
    "    cur_letter = first_letter\n",
    "    while len(seq) < length:\n",
    "        # iteratively append to the sequence according to bigram frequency\n",
    "        bigram_freq = bigram[(\"ABC\").index(cur_letter)]\n",
    "        \n",
    "        # draw next letter from the given bigram distribution\n",
    "        next_letter = np.random.choice(list(\"ABC\"), p=bigram_freq)\n",
    "        seq += next_letter\n",
    "        cur_letter = next_letter\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 80\n",
    "bigram2 = np.asarray([[0.1, 0.8, 0.1], [0.6, 0.2, 0.2], [0.3, 0.3, 0.4]])\n",
    "neg_bigram2 = np.asarray([[0.6, 0.3, 0.1], [0.1, 0.2, 0.7], [0.3, 0.5, 0.2]])\n",
    "np.random.seed(1)\n",
    "training_seq = draw_samples_MP_three_letters(bigram2, seq_len, 10000)\n",
    "\n",
    "negatives = draw_samples_MP_three_letters(neg_bigram2, seq_len, 1000)\n",
    "positives = draw_samples_MP_three_letters(bigram2, seq_len, 1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "one_hot = [[0, 0, 1], [0, 1, 0], [1, 0, 0]]\n",
    "\n",
    "training_seq = np.asarray([[one_hot[(\"ABC\").index(y)] for y in x] for x in training_seq])\n",
    "print(training_seq.shape)\n",
    "training_seq = training_seq.reshape(training_seq.shape[0], seq_len*3)\n",
    "\n",
    "positives = np.asarray([[one_hot[(\"ABC\").index(y)] for y in x] for x in positives])\n",
    "positives = positives.reshape(positives.shape[0], seq_len*3)\n",
    "\n",
    "negatives = np.asarray([[one_hot[(\"ABC\").index(y)]for y in x] for x in negatives])\n",
    "negatives = negatives.reshape(negatives.shape[0], seq_len*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "rich_training = expand_rich_no_linear(training_seq)\n",
    "rich_pos = expand_rich_no_linear(positives)\n",
    "rich_neg = expand_rich_no_linear(negatives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel: linear, nu: 1e-05, result:  0.956\n",
      "true pos:  913\n",
      "true neg:  999\n",
      "kernel: linear, nu: 2e-05, result:  0.956\n",
      "true pos:  913\n",
      "true neg:  999\n",
      "kernel: linear, nu: 0.0001, result:  0.956\n",
      "true pos:  913\n",
      "true neg:  999\n",
      "kernel: linear, nu: 0.0002, result:  0.956\n",
      "true pos:  913\n",
      "true neg:  999\n",
      "kernel: linear, nu: 0.001, result:  0.956\n",
      "true pos:  913\n",
      "true neg:  999\n",
      "kernel: linear, nu: 0.002, result:  0.956\n",
      "true pos:  913\n",
      "true neg:  999\n",
      "kernel: linear, nu: 0.01, result:  0.956\n",
      "true pos:  913\n",
      "true neg:  999\n",
      "kernel: linear, nu: 0.02, result:  0.957\n",
      "true pos:  915\n",
      "true neg:  999\n",
      "kernel: linear, nu: 0.1, result:  0.936\n",
      "true pos:  873\n",
      "true neg:  999\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "with open('result_simulation.txt', 'a') as file:\n",
    "    file.write(\"Experiment 5.2 linear kernel len = 2\")\n",
    "    date_time = datetime.now().strftime(\"%m/%d/%Y, %H:%M:%S\")\n",
    "    file.write(date_time)\n",
    "    file.write('\\n\\n')\n",
    "    # train a linear model, which should theoretically be similar to the potts model.\n",
    "    for n in [0.00001, 0.00002, 0.0001, 0.0002, 0.001, 0.002, 0.01, 0.02, 0.1]:\n",
    "        clf = OneClassSVM(kernel='linear', nu=n)\n",
    "        clf.fit(rich_training)\n",
    "        pred_pos = clf.predict(rich_pos)\n",
    "        pred_neg = clf.predict(rich_neg)\n",
    "        \n",
    "        true_pos = np.count_nonzero(pred_pos == 1)\n",
    "        true_neg = np.count_nonzero(pred_neg != 1)\n",
    "        result = (true_neg + true_pos)/(pred_neg.shape[0] + pred_pos.shape[0])\n",
    "        print(f'kernel: linear, nu: {n}, result: ', result)\n",
    "        print(\"true pos: \", true_pos)\n",
    "        print(\"true neg: \", true_neg)\n",
    "        file.write(f'kernel: linear, nu: {n}, result: {result}\\n')\n",
    "        file.write(f'True Positives: {true_pos}, True Negatives: {true_neg}\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel: linear, nu: 0.7, result:  0.6635\n",
      "true pos:  477\n",
      "true neg:  850\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "n = 0.7\n",
    "clf = OneClassSVM(kernel='linear', nu=n)\n",
    "clf.fit(rich_training)\n",
    "pred_pos = clf.predict(rich_pos)\n",
    "pred_neg = clf.predict(rich_neg)\n",
    "\n",
    "true_pos = np.count_nonzero(pred_pos == 1)\n",
    "true_neg = np.count_nonzero(pred_neg != 1)\n",
    "result = (true_neg + true_pos)/(pred_neg.shape[0] + pred_pos.shape[0])\n",
    "print(f'kernel: linear, nu: {n}, result: ', result)\n",
    "print(\"true pos: \", true_pos)\n",
    "print(\"true neg: \", true_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.intercept_ += 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.19234755e-09,  0.00000000e+00,  1.19234755e-09,  0.00000000e+00,\n",
       "        1.19234755e-09,  1.19234755e-09,  1.19234755e-09, -2.99400000e+03,\n",
       "        1.19234755e-09,  0.00000000e+00,  0.00000000e+00,  1.19234755e-09,\n",
       "       -3.02600000e+03,  1.19234755e-09,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(rich_pos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/arthur/dev/msa_classify/simulation_markov.ipynb Cell 49\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bwhale.bair.berkeley.edu/home/arthur/dev/msa_classify/simulation_markov.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m coef \u001b[39m=\u001b[39m clf\u001b[39m.\u001b[39mcoef_\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bwhale.bair.berkeley.edu/home/arthur/dev/msa_classify/simulation_markov.ipynb#X66sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(coef)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clf' is not defined"
     ]
    }
   ],
   "source": [
    "coef = clf.coef_\n",
    "print(coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        1.68984116e-09,  0.00000000e+00,  1.68984116e-09,  0.00000000e+00,\n",
       "        1.68984116e-09,  1.68984116e-09,  1.68984116e-09, -3.99400000e+03,\n",
       "        1.68984116e-09,  0.00000000e+00,  0.00000000e+00,  1.68984116e-09,\n",
       "       -4.02600000e+03,  1.68984116e-09,  0.00000000e+00,  0.00000000e+00])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.decision_function(rich_pos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1, -1, -1, -1,  1, -1,  1, -1,  1,  1,  1, -1,  1, -1, -1,  1, -1,\n",
       "        1, -1, -1])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predict(rich_pos[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rich_pos[:20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiclass SVM with positive data and random negative data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 ('dev_arthur')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b975e03f1e86e078ea1d566012707067ac1f6e4c759ff48317f6f9dc08e1449"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
